<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Open Spirits]]></title>
  <link href="http://pawa.github.com/atom.xml" rel="self"/>
  <link href="http://pawa.github.com/"/>
  <updated>2013-09-04T23:23:25+08:00</updated>
  <id>http://pawa.github.com/</id>
  <author>
    <name><![CDATA[Stone Wang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[如何成为一名靠谱的工程师]]></title>
    <link href="http://pawa.github.com/blog/2013/09/04/reliable-engineer/"/>
    <updated>2013-09-04T23:11:00+08:00</updated>
    <id>http://pawa.github.com/blog/2013/09/04/reliable-engineer</id>
    <content type="html"><![CDATA[<p>这文章来自刚才写给公司内部工程师同学的一封内部邮件，希望也分享给其他感兴趣的朋友。</p>

<p>背景介绍：雪球（http://xueqiu.com）是一个投资者社交平台，我们的架构是前端Web/移动端都共同基于后端的 API 来进行开发，因为前端后端不是同一个人来开发，在进度和思路上肯定有些偏差，就存在一个相互配合的问题，在我们不期望有所谓“领导”每天追进度的环境下还是会发生一些不可控的事情，今天发生的事还是一个比较典型的例子，下边是邮件的正文，未删减 &#8230;</p>

<p>晚上大家讨论中的的焦点很大一部分集中在进度上，而进度问题大家的观点是因为各种原因后端的接口没有很完备，设计、实现都出现了一些问题，这已经快要到已经推迟过的一个时间节点的时候接口出现的一些错误和不完备让大家突然对项目按时交付出现了信心不足。</p>

<p>从后端的同学来看接口出现的都是一些小错误，这些小错误只要在真正被使用时候才能够被捕捉到，而有一些错误是因为我们使用的框架实在是很容易写出一些问题代码，这些都是很容易修复的问题，只要有人告诉。但是前端和产品的同学显然不会这么想，第一感觉肯定是说因为后端接口有问题我们没办法有效的测试，只能让后端改了我才能测试，界面其实都写好了，因为出现了各种小问题，所以天然觉得任何地方都可能出问题啊，根本觉得后端完全不靠谱了。</p>

<p>现在的问题就是，如果才能作为后端工程师不被前端的同学抱怨呢？核心的思路是明确自己的职责想清楚要服务的对象。比如我们整个产品服务的对象是广大投资者而不是产品经理；后端工程师的接口是要服务于前端和移动的工程师，未来还要服务第三方合作伙伴；运维的同学是要服务于开发的同学。所有这一切要以服务大爷的思路去考虑问题，要让大爷爽，让大爷没话说，让大爷恨不得给赏钱。具体到今天这个案例，来看看后端工程师的大爷们（前端和移动工程师）有啥需求？</p>

<ol>
<li>接口文档清晰准确</li>
<li>接口实现稳定不出错</li>
<li>按照商定的日期发布接口</li>
<li>有问题及时响应</li>
</ol>


<p>这几个需求如何满足又可以从技术和非技术层面去推动解决：</p>

<p>技术上来说，很多问题靠人为保证往往不靠谱，正常的人每个月总会有那么几天精神不佳，比如忘记这个判断，那个校验，或者代码更新了接口没有更新，如果通过一些框架方面的基础设施可以解决这些问题那显然可以大大降低低级失误出现的概率，如果现有框架实在没办法满足快速稳定的产出新功能，可以考虑逐步替换重构的方式去解决这个问题，而不是10年8年逆来顺受的被折磨。</p>

<p>非技术上来说，如果觉得确实有一些限制没办法做到很完备的测试，需要别人帮助那一定要主动的请求别人的帮助，请别人帮忙检查代码，请别人尽快测试接口，每个人都有很多事，不一定会把你提出的事第一位考虑，但作为一个靠谱工程师目标是为了尽快确认自己的接口是没有问题的，那么请求别人帮助是其中非常重要的途径之一，为什么不能主动去请别人尽快完成自己的帮助请求呢？一次不行两次，两次不行天天催，丫肯定受不了你的就从了。这样做了肯定不会出现快要到时间了还没有人甚至自己也不清楚各种功能到底行不行的问题。</p>

<p>这里或许有方法或许有态度，也可以推广到各种职责的同学，总之如果都这么搞了，我敢打百分百包票，就算你现在不是经验那么丰富能力那么强的工程师，别人也一定会认为你靠谱，如果你每次都这么靠谱，你一定可以成为大牛，成为别人可以信赖得人。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Qcon Beijing 2013]]></title>
    <link href="http://pawa.github.com/blog/2013/04/25/qcon-beijing-2013/"/>
    <updated>2013-04-25T22:44:00+08:00</updated>
    <id>http://pawa.github.com/blog/2013/04/25/qcon-beijing-2013</id>
    <content type="html"><![CDATA[<p>一年一度的 Qcon Beijing 还是在四月底开始了，北京终于天气暖和了空气质量也有改善了，咳咳咳。还是记录一下这几天参会的感想吧。</p>

<h3>Github</h3>

<p>Github 的运维工程师 Jesse Newland (某人非说此哥们名字是个女人名，刚查了一下 Jesse 男女都能用 Jessie 才是女人名) 介绍了他们在运维过程中使用的一个自己开发的 Hubot 工具，这是一个聊天室，所有的工程师都在里边，还多了一个机器人就叫做 hubot &#8230;</p>

<p>可以让这个机器人去部署代码，获取服务器的状态图，操作负载均衡关闭某个节点 &#8230; 总之是任何运维的事他都能干，这玩意开源了 <a href="http://hubot.github.com/">http://hubot.github.com/</a> 是 CoffeeScript + Node.js 写的。</p>

<p>这个机器人可以跑在很多环境下，内置了 Campfire 和 Shell，社区还贡献了其他各种各样的 adapter，什么 IRC / Twitter / XMPP / Gtalk 等等，咱也可以整一个 Xueqiu 的，然后私信给他就可以让这机器人帮咱运维鸟 &#8230;</p>

<p>说到为什么要用这个有一大堆原因，我印象深刻的是这三个：</p>

<ul>
<li>Teaching by doing</li>
<li>Communicate by doing</li>
<li>Things I haven&#8217;t asked recently</li>
</ul>


<p>还有个有意思的是有人问到权限问题，因为他们所有人都在这个聊天室，理论上谁都可以执行那些命令，怎么搞，哥们说了，基于信任，这个这个 &#8230; 恩恩</p>

<p>还有一场 Jesse 讲 Github 架构，很简单，LVS / Nginx / Unicorn / Redis / Memcached / Mysql 他们是 Ruby 所以 App Server 是 Unicorn 除了这个其他语言换成相应的 Server 这个架构几乎是所有互联网产品的标准配置了。当然他们比较特殊的地方是因为要 Host Git Repo 所以在 Git 和 SSH 上做了很多的改造来适应他们的应用场景。</p>

<p>最后 Jesse 还介绍了一下当时处理 12306 插件搞垮 Github 的事，凌晨4点他被报警吵醒，我其实觉得他到现在也不是很理解为啥那么多人买票，而且有那么多观光团去他发的那个问题下边评论，哈哈哈</p>

<h3>安全宝</h3>

<p>道哥从阿里去了安全宝，就冲着云端WAF去的应该是，他介绍了 WAF 的发展路径开源产品->商业硬件->云服务。</p>

<p>因为 WAF 的复杂性和专业性花钱买保险还是很有必要的，只要有靠谱云服务，不要像前段时间 CloudFlare 坏掉影响78万客户1小时无法访问这种重大事故的情况下，我觉得还是可以尝试使用的。</p>

<h3>沃趣</h3>

<p>这是前阿里的几个 DBA/SA 同学的创业公司，主要做数据库方面的产品和服务，据说是很牛的一群人。主要介绍了一下最近几年比较常见的 Oracle / Mysql 的数据库架构，可能没有特别多新东西，但是对于数据库这部分有可能演进的路线更加清晰了。硬件的更新 HDD -> SSD -> PCIe 甚至可以满足绝大多数情况下可以满足要求了，这是 Scale Up。如果这样也不行了，就分库分表，加数据库访问层。这些之外就是高可用、备份、SQL优化等基础工作了。</p>

<h3>Jing.fm</h3>

<p>创始人是中俄混血，学古典音乐的，要实现一个非常理想化状态的音乐产品，要学习音乐的各种属性，并且根据这些属性来进行分类，让用户通过语意化的输入来推荐，而且是可以理解不同人对各种情绪的状态进行相对硬的匹配，就是说你的悲伤和我的悲伤是不同的悲伤这个产品也试图能够识别。同去的都是工程师同学，从工程师的角度看这个产品，觉得这产品简直太理想了，难度太大了，各个方面似乎都不是小型创业团队能搞定的事情。不过以不被鄙视的事是不值得去做的这个观点看，这事显然是很值得去做的，而且只能靠这跨界的同学去搞了。</p>

<h3>有道云笔记</h3>

<p>各种数据，没记太清，好像用户是1000万，后台技术采用了有道自己开发的分布式存储、分析、查询的底层技术，然后在上边做了很多封装，整体架构非常复杂，对于一个笔记产品似乎有过度设计的嫌疑。PC客户端/移动客户端还是用了不同的 API 接口，这个更不是很理解了，是人多还是真不怕麻烦？总体感觉是，如果一家创业公司这么搞，死的会很惨啊 &#8230;</p>

<h3>总体感觉</h3>

<p>大家都感觉这种技术会议有点水了，可能并不是水了，不知道是不是最近几年各种会议太多了，再加上各种社交媒体导致很多技术方向架构平常都有很多的了解，反而在这种大会上的演讲在有限时间内反而不太容易像前几年那样有特别多的全新信息了，到还是会上碰到的朋友闲聊有不少有意思的收获和共鸣。</p>

<p>这次感觉最深的是，相对于国内一些大公司的产品、架构来说，对于像雪球这样的小型创业公司来说我们的学习对象是 Github 这种，使用最常用的开源产品和解决方案、用最简单的架构、自己去开发各种合适的提高效率的小工具。</p>

<p>还有国内好像并没有一个专门针对中小型互联网公司的技术会议，其实对于中小型的公司来说，关注的点更分散一些，用什么Web服务器、用什么缓存服务器、服务器配置是什么样的、工程管理是什么样的、怎么做代码审查、怎么做测试、怎么招人这样的问题无数啊，而从大公司来的经验往往都不是很适合的，如果有类似的交流会真很愿意参加。</p>

<h3>雪球找人</h3>

<p>公司介绍：<a href="http://xueqiu.com/about/what-is-xueqiu">http://xueqiu.com/about/what-is-xueqiu</a></p>

<p>职位介绍：<a href="http://www.snowballfinance.com/job.html">http://www.snowballfinance.com/job.html</a></p>

<p>也可以直接发我邮箱 wangdong # xueqiu.com ，牛们，没有职位我们可以创造职位！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Plantyoffish: one man handled site and worth billion dollars]]></title>
    <link href="http://pawa.github.com/blog/2012/11/26/plantyoffish-one-man-handled-and-worth-1b-dollars/"/>
    <updated>2012-11-26T17:32:00+08:00</updated>
    <id>http://pawa.github.com/blog/2012/11/26/plantyoffish-one-man-handled-and-worth-1b-dollars</id>
    <content type="html"><![CDATA[<p><a href="http://highscalability.com/blog/2012/11/22/gone-fishin-plentyoffish-architecture.html">Gone Fishin&#8217;: PlentyOfFish Architecture</a></p>

<p>一个不算新的旧闻，被 <a href="http://highscalability.com">Highscalability</a> 翻出来了。PlantyOfFish 不算是个特有名的网站，有媒体报道这网站值$10亿刀，这网站数据不小，这还只是一年前的数据：</p>

<ul>
<li>Windows 技术</li>
<li>每月60亿PV</li>
<li>320亿图片</li>
<li>每日600万登录用户</li>
<li>11台 Web Server（其中5台完全可以不需要）</li>
<li>CTR 是 FB 的5-10倍</li>
<li>每月托管费用$7万刀不到</li>
</ul>


<p>更神奇的是：</p>

<ul>
<li>只有俩人(Founder和一个DBA)，每天工作2小时</li>
<li>每年用 Google Adsense 挣 $1000 万</li>
</ul>


<p>好的产品用心做真的是可以挣钱的，技术也不是什么不可以逾越的障碍，其实这个网站一直是采取 Scale Up 的方式在扩展（人少不愿改代码来 Scale Out），所以如何实用的解决问题最重要。</p>

<p>新功能从来都是想了就做，做了就上，观察各项数据是否增长，用户是否喜欢，不好了就下，人多不一定真的是优势！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Day 2 of SDCC 2012]]></title>
    <link href="http://pawa.github.com/blog/2012/09/09/day-2-of-sdcc-2012/"/>
    <updated>2012-09-09T23:18:00+08:00</updated>
    <id>http://pawa.github.com/blog/2012/09/09/day-2-of-sdcc-2012</id>
    <content type="html"><![CDATA[<h3>耗子哥</h3>

<p>CoollShell 的作者，一个非常有观点的同学，反对敏捷 / 反对不写代码的专家 / C++ 脑残粉，我倒是很赞同他的有观点比没观点总是要好。他的题目比较大，高并发互联网应用，基本上每个 Slide 都可以来讲2小时，所以没法深入，可以当个提纲参考。</p>

<h3>Netflix</h3>

<p>又是 Netflix，今天是详细介绍他们的云平台，所谓这个云平台其实就是在 AWS 的 IaaS 基础上开发了一个自己的 PaaS， 实际上底层的实现是无关应用开发的，也就是说哪天他们想自己做 IDC 了，只需要重新实现一些 AWS 特定的实现，应用完全不用修改。这个平台里有的模块都是与具体业务实现无关的抽象和总结，比如：服务注册、配置管理、日志收集、资源调度。后来有一部分因为有个线上 BUG 要处理，没听到，有 PPT 后再重温一下。</p>

<h3>Amazon</h3>

<p>大型软件自动化部署，这个相当感兴趣，分析的很深入。几个印象深刻的地方：1. 统一部署概念（任何安装都是部署） 2. 部署=搬家 3. DevOps 是改进是不起作用的，要创新革命</p>

<h3>丁香园</h3>

<p>大辉同学介绍了2年来对丁香园的一些改进，有坑也有成功，对中小创业团队很有帮助，我的感觉是原来大家碰到的问题都一样。其实最想交流的是团队建设、安全防护、反垃圾方面的实践。会后和大辉换了名片，他太忙，没顾上聊，他倒是赞了雪球的产品，交流了几个产品问题，非常感谢他的支持。希望以后有机会深入交流。</p>

<h3>总体感觉</h3>

<p>这两天的会还是有不少收获，特别是有两个最近思考的问题似乎有了点思路</p>

<ol>
<li><p>现阶段我们是否真的需要专职的运维工程师以及如何做运维工作，似乎答案很明确了 - <strong>不需要</strong>。受 Amazon 同学演讲的启发，他们认为互联网产品实际上不需要 QA 也不需要 Ops，我认为确实是对的。就像 “Google 如何测试” 那个系列描述的 QA 的职责是维护自动化测试框架、工具以及指导监督开发人员进行具体测试开发一样，Ops 也应该是去维护自动化运维的流程、工具、框架以及指导开发人员进行具体的日常运维。因为只有开发人员自己才最了解业务，最了解需求，最了解哪里最可能出现问题，也最可能最快定位和解决问题。如果他们都能自助的解决问题，为啥非要和另外一个角色去沟通交流呢，完全没必要。所以就算我们需要专业的运维工程师，也肯定是需要有非常丰富开发经验的工程师来完成上边描述的事。</p></li>
<li><p>如何更有效的开发、测试、部署，答案应该就是 - <strong>虚拟化</strong>。Twitter / Netflix / Alipay / Douban 都在做类似的事情，尽管他们的实现方式千差万别，不过核心的思路是，提供统一的开发框架、统一的开发测试环境、统一的部署环境，部署就是把开发测试的环境搬个家而已。感觉要实现类似的效果唯有实现自己的 PaaS 平台，屏蔽开发者对于底层服务的安装配置，高度抽象提取公共模块，统一系统框架（开发、运维方式），至于是通过虚拟机方式还是 Mesos 的资源共享方式，还有待研究，好在各种方式都有可以参考的对象。</p></li>
</ol>


<h3>加个广告</h3>

<p>以上我描述的解决问题的方法，或早或晚都会成为我们公司产品架构的一部分，如果有任何同学感兴趣可以随时联系我 wangdong # xueqiu.com</p>

<p>至于我们公司的情况请猛击：<a href="http://xueqiu.com/about/what-is-xueqiu">http://xueqiu.com/about/what-is-xueqiu</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Day 1 of SDCC 2012]]></title>
    <link href="http://pawa.github.com/blog/2012/09/08/day-1-of-sdcc-2012/"/>
    <updated>2012-09-08T23:42:00+08:00</updated>
    <id>http://pawa.github.com/blog/2012/09/08/day-1-of-sdcc-2012</id>
    <content type="html"><![CDATA[<p>周末两天参加 SDCC 2012 的会议，有些有意思的见闻和想法。</p>

<h3>上午</h3>

<p>一句话简介：垃圾时间，木啥收获</p>

<h3>Twitter</h3>

<p>一位 Twitter 的华裔女工程师，当然是一口流利的普通话，话说这在历次参加过的技术会议上还是比较少见的。</p>

<p>讲的内容是关于 Twitter 的架构演进，从一个全功能的 App 逐步走向模块化的 Service，为了 Service 的方便开发、管理、追踪开发了一系列的基础设施和框架，而且都开源了，这些个东西以前多少都看过这次是更加深了印象。为了开发这些 Service 他们还基本上从 Ruby 迁移到了 Jvm 上（现在应该绝大多数是 Scala，印象中也有 Clojure 了，不确定）。最后还提到引入了 Mesos，来做虚拟化资源管理，提高资源的利用率（记得今年 Qcon 时候，豆瓣的 hongqn 也提到他们在做 dae 中也在使用 Mesos）。</p>

<p>演讲后还和她简单交流了一下，问了俩问题，也挺有启发</p>

<ol>
<li><p>如何保证模块化之间的正确依赖，答案是：紧密的模块都在同一个 Repo 会一起编译，并且运行相应的测试代码，如果是 RPC 调用，只要 Thrift 接口的版本是一致的，也不会有问题，单元测试是比较全面的不过针对 RPC 之间的测试并没有那么完善。</p></li>
<li><p>如何做集成测试，答案是：利用 Iago 复制产品环境的流量，然后 Replay 去测试一个完整的测试集群。</p></li>
</ol>


<h3>Netflix</h3>

<p>因为 Netflix 全部基于 Amaonz 的 AWS 服务，所以 Build/Test 都是是以虚拟机的镜像文件为单位的，加上他们自己的云管理平台，极大的简化了测试和部署的繁琐。另外 Netflix 有两个比较特别的制度，引起在场屌丝们的议论，特别是第一个</p>

<ol>
<li><p>无限制休假 - 任何时间任何长度的休假都由自己决定，当然前提是不影响工作以及有问题随时中止休假。我觉得似乎做到这个不是很难吧，不管是公司方面还是雇员，特别对于工程师，其实本质上和在家或者在什么地方远程工作没啥区别，反正不是写这些代码就是写那些代码，不同的是在哪写而已。</p></li>
<li><p>无职位差别，都是 Senior Engineer 进公司就这样，出去还这样，这个好多人没啥感觉，但我觉得这个才是个厉害的招数，非常的反常轨，一般可能只有几个人的公司才做得到。对于一家上市的成熟企业还是很难的，这样的好处是找到的愿意加入的人都是愿意 Coding 的，对于 Title 没啥追求，所以一群热爱 Coding 的人自然能做到很牛的事，真是印象深刻这一点。</p></li>
</ol>


<h3>支付宝</h3>

<p>架构也是从 单一应用 -> 面向服务 -> 云平台 这样的演进，看来雪球还处于最初阶段呢，有的是折腾的空间 :p</p>

<p>同时他们的架构也异常复杂，可能由于流量特别大，业务特别复杂导致的，所以 90% 的公司永远都不会用到那么复杂的架构，又或许应该还有更简单的方法？没有经历过那种场景还是无法深刻的理解。对于一天要支持1亿笔交易的目标，还是觉得非常震撼和佩服。</p>

<h3>虚拟化</h3>

<p>Twitter / Netflix / 支付宝 都在虚拟化(或者所谓的云平台)上做了技术上的投资，我还是更愿意说虚拟化而不是什么云。这个趋势是实实在在的为了解决很多问题才流行的而不是 buzzword 驱使。</p>

<p>最直接解决的生产环境问题是 1. 最大化利用硬件资源 2. 自动化的错误检测/处理/升级。除了这些，我想虚拟化还可以应用在开发/测试/持续集成等所有的开发活动中来简化开发环境的配置并且更真实的模拟生生产环境。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu 10.04.3 kernel bug]]></title>
    <link href="http://pawa.github.com/blog/2012/05/17/ubuntu-10-dot-04-dot-3-kernel-bug/"/>
    <updated>2012-05-17T10:29:00+08:00</updated>
    <id>http://pawa.github.com/blog/2012/05/17/ubuntu-10-dot-04-dot-3-kernel-bug</id>
    <content type="html"><![CDATA[<p>运行了200天的 Ubuntu 10.04.3 在没有任何征兆的情况下突然系统负载(Load)狂高，Java 进程完全不能启动，但是有些服务 MySQL / Redis 等却不受影响，因为害怕影响线上的服务，所以没有全部重启，看着系统负载从10一直飙升到500，后来干脆连 SSH 也进不去了。最后只能悲剧的重启，重启后一切恢复正常。</p>

<p>查看日志发现一些内核的报错</p>

<pre><code>May 16 10:07:16 w2 kernel: [ 8985.860328] Call Trace:
May 16 10:07:16 w2 kernel: [ 8985.860339]  [&lt;ffffffff8121b4ea&gt;] ? jbd2_journal_dirty_metadata+0x10a/0x140
May 16 10:07:16 w2 kernel: [ 8985.860345]  [&lt;ffffffff8153fc5d&gt;] schedule_timeout+0x22d/0x300
May 16 10:07:16 w2 kernel: [ 8985.860350]  [&lt;ffffffff8104b3c9&gt;] ? sched_slice+0x59/0xa0
May 16 10:07:16 w2 kernel: [ 8985.860355]  [&lt;ffffffff8115dea3&gt;] ? dup_fd+0x33/0x340
May 16 10:07:16 w2 kernel: [ 8985.860360]  [&lt;ffffffff812b43d3&gt;] ? cpumask_next_and+0x23/0x40
May 16 10:07:16 w2 kernel: [ 8985.860363]  [&lt;ffffffff8153f87b&gt;] wait_for_common+0xdb/0x180
May 16 10:07:16 w2 kernel: [ 8985.860367]  [&lt;ffffffff8105ddcb&gt;] ? enqueue_task_fair+0x5b/0xa0
May 16 10:07:16 w2 kernel: [ 8985.860371]  [&lt;ffffffff8105ccb0&gt;] ? default_wake_function+0x0/0x20
May 16 10:07:16 w2 kernel: [ 8985.860374]  [&lt;ffffffff8153f9dd&gt;] wait_for_completion+0x1d/0x20
May 16 10:07:16 w2 kernel: [ 8985.860378]  [&lt;ffffffff81065870&gt;] do_fork+0x150/0x440
May 16 10:07:16 w2 kernel: [ 8985.860381]  [&lt;ffffffff8115ff40&gt;] ? mntput_no_expire+0x30/0x110
May 16 10:07:16 w2 kernel: [ 8985.860387]  [&lt;ffffffff8107e085&gt;] ? set_one_prio+0x75/0xd0
May 16 10:07:16 w2 kernel: [ 8985.860391]  [&lt;ffffffff8101a085&gt;] sys_vfork+0x25/0x30
May 16 10:07:16 w2 kernel: [ 8985.860396]  [&lt;ffffffff81012513&gt;] stub_vfork+0x13/0x20
May 16 10:07:16 w2 kernel: [ 8985.860399]  [&lt;ffffffff810121b2&gt;] ? system_call_fastpath+0x16/0x1b 
</code></pre>

<p>Google 了一下，发现果然是个悲剧，装服务器时候使用了 Ubuntu-10.04.3-amd64 版本，装出来的 Kernel 版本是 2.6.32-33-generic，至少应该是 2.6.32-33-server 啊，这两者的差别可以参考下边链接的描述，总之服务器上用上了 Desktop 的 Kernel 肯定是有问题的。</p>

<p><a href="http://jaseywang.me/2011/09/19/generic-kernel-%E5%92%8C-server-kernel/">http://jaseywang.me/2011/09/19/generic-kernel-%E5%92%8C-server-kernel/</a></p>

<p>后来又发现就算是 2.6.32-33-server 也还是有 bug 的，所有的特征都符合</p>

<p><a href="http://jaseywang.me/2012/05/07/2-6-32-33-%E5%86%85%E6%A0%B8%E8%B7%91%E5%88%B0-208-%E5%A4%A9%E5%AE%95%E6%9C%BA">http://jaseywang.me/2012/05/07/2-6-32-33-%E5%86%85%E6%A0%B8%E8%B7%91%E5%88%B0-208-%E5%A4%A9%E5%AE%95%E6%9C%BA</a></p>

<p>解决办法就是升级内核，对于 10.04，选择用最新的 Ubuntu-10.04.4-amd6 吧，顺便把内核升级到 2.6.32-41-server 吧，以前的某些服务器用的是这个内核一直很稳定。在安装服务器时候还是要小心谨慎!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[First Octopress Blog]]></title>
    <link href="http://pawa.github.com/blog/2012/04/11/first-octopress-blog/"/>
    <updated>2012-04-11T22:51:00+08:00</updated>
    <id>http://pawa.github.com/blog/2012/04/11/first-octopress-blog</id>
    <content type="html"><![CDATA[<p>用上了 github + octopress</p>
]]></content>
  </entry>
  
</feed>
